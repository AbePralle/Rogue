$[include "RogueError.bard"]

class Tokenizer
  PROPERTIES
    filepath    : String
    reader      : ParseReader
    tokens      = Token[]
    buffer      = StringBuilder()

    next_filepath : String
    next_line     : Integer
    next_column   : Integer

  METHODS
    method tokenize( filepath )->Token[]
      return tokenize( ParseReader(filepath) )

    method tokenize( filepath, data:String )->Token[]
      return tokenize( ParseReader(filepath, data) )

    method tokenize( reader )->Token[]
      configure_token_types
      while (tokenize_another) noAction

      if (tokens.count == 0)
        # Ensure there's an EOL token at the end.
        if (tokens.count == 0 or tokens.last.type isNot TokenType.eol)
          add_new_token( TokenType.eol )
        endIf
      endIf

      return tokens

    # -------------------------------------------------------------------------

    method add_new_string_or_character_token_from_buffer( terminator:Character )->Logical
      # We have a string in 'buffer' already; convert it to a character if it has count 1.
      if (buffer.count == 1 and terminator == '\'')
        return add_new_token( TokenType.literal_character, buffer[0] )
      else
        return add_new_token( TokenType.literal_string, buffer->String )
      endIf

    method add_new_token( type:TokenType )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column) )
      return true

    method add_new_token( type:TokenType, value:Character )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Integer )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Real )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:String )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method configure_token_types
      if (TokenType.lookup?) return

      TokenType.lookup        = Table<<String,TokenType>>()

      # Directives
      TokenType.directive_define  = define( DirectiveTokenType("$define") )
      TokenType.directive_include = define( DirectiveTokenType("$include") )
      TokenType.directive_if      = define( DirectiveTokenType("$if") )
      TokenType.directive_elseIf  = define( DirectiveTokenType("$elseIf") )
      TokenType.directive_else    = define( DirectiveTokenType("$else") )
      TokenType.directive_endIf   = define( DirectiveTokenType("$endIf") )

      TokenType.placeholder_id    = define( TokenType("$id") )

      # Class Structure Tokens
      TokenType.eol                = define( EOLTokenType("end of line") )
      TokenType.keyword_class      = define( ClassStructureTokenType("class") )
      TokenType.keyword_endClass   = define( ClassStructureTokenType("endClass") )
      TokenType.keyword_method     = define( ClassStructureTokenType("method") )
      TokenType.keyword_METHODS    = define( ClassStructureTokenType("METHODS") )
      TokenType.keyword_PROPERTIES = define( ClassStructureTokenType("PROPERTIES") )

      # Statement Tokens
      TokenType.keyword_println    = define( TokenType("println") )

      # Expression Tokens
      TokenType.identifier                   = define( TokenType("identifier") )
      TokenType.literal_character            = define( TokenType("Character") )
      TokenType.literal_integer              = define( TokenType("Integer") )
      TokenType.literal_real                 = define( TokenType("Real") )
      TokenType.literal_string               = define( TokenType("String") )

      TokenType.keyword_and                  = define( TokenType("and") )
      TokenType.keyword_false                = define( TokenType("false") )
      TokenType.keyword_not                  = define( TokenType("not") )
      TokenType.keyword_or                   = define( TokenType("or") )
      TokenType.keyword_true                 = define( TokenType("true") )
      TokenType.keyword_xor                  = define( TokenType("xor") )

      TokenType.symbol_ampersand             = define( TokenType("&") )
      TokenType.symbol_ampersand_equals      = define( TokenType("&=") )
      TokenType.symbol_arrow                 = define( TokenType("->") )
      TokenType.symbol_at                    = define( TokenType("@") )
      TokenType.symbol_backslash             = define( TokenType("\\") )
      TokenType.symbol_caret                 = define( TokenType("^") )
      TokenType.symbol_caret_equals          = define( TokenType("^=") )
      TokenType.symbol_close_brace           = define( TokenType("}") )
      TokenType.symbol_close_bracket         = define( TokenType("]") )
      TokenType.symbol_close_comment         = define( TokenType("}#") )
      TokenType.symbol_close_paren           = define( TokenType(")") )
      TokenType.symbol_close_specialize      = define( TokenType(">>") )
      TokenType.symbol_colon                 = define( TokenType(":") )
      TokenType.symbol_colon_colon           = define( TokenType("::") )
      TokenType.symbol_comma                 = define( TokenType(",") )
      TokenType.symbol_compare               = define( TokenType("<>") )
      TokenType.symbol_dot                   = define( TokenType(".") )
      TokenType.symbol_dot_equals            = define( TokenType(".=") )
      TokenType.symbol_downToGreaterThan     = define( TokenType("..>") )
      TokenType.symbol_empty_braces          = define( TokenType("{}") )
      TokenType.symbol_empty_brackets        = define( TokenType("[]") )
      TokenType.symbol_eq                    = define( TokenType("==") )
      TokenType.symbol_equals                = define( TokenType("=") )
      TokenType.symbol_exclamation_point     = define( TokenType("!") )
      TokenType.symbol_fat_arrow             = define( TokenType("=>") )
      TokenType.symbol_ge                    = define( TokenType(">=") )
      TokenType.symbol_gt                    = define( TokenType(">") )
      TokenType.symbol_le                    = define( TokenType("<=") )
      TokenType.symbol_lt                    = define( TokenType("<") )
      TokenType.symbol_minus                 = define( TokenType("-") )
      TokenType.symbol_minus_equals          = define( TokenType("-=") )
      TokenType.symbol_minus_minus           = define( TokenType("--") )
      TokenType.symbol_ne                    = define( TokenType("!=") )
      TokenType.symbol_open_brace            = define( TokenType("{") )
      TokenType.symbol_open_bracket          = define( TokenType("[") )
      TokenType.symbol_open_paren            = define( TokenType("(") )
      TokenType.symbol_open_specialize       = define( TokenType("<<") )
      TokenType.symbol_percent               = define( TokenType("%") )
      TokenType.symbol_percent_equals        = define( TokenType("%=") )
      TokenType.symbol_plus                  = define( TokenType("+") )
      TokenType.symbol_plus_equals           = define( TokenType("+=") )
      TokenType.symbol_plus_plus             = define( TokenType("++") )
      TokenType.symbol_question_mark         = define( TokenType("?") )
      TokenType.symbol_semicolon             = define( TokenType(";") )
      TokenType.symbol_shift_left            = define( TokenType(":<<:") )
      TokenType.symbol_shift_right           = define( TokenType(":>>:") )
      TokenType.symbol_shift_right_x         = define( TokenType(":>>>:") )
      TokenType.symbol_slash                 = define( TokenType("/") )
      TokenType.symbol_slash_equals          = define( TokenType("/=") )
      TokenType.symbol_tilde                 = define( TokenType("~") )
      TokenType.symbol_tilde_equals          = define( TokenType("~=") )
      TokenType.symbol_times                 = define( TokenType("*") )
      TokenType.symbol_times_equals          = define( TokenType("*=") )
      TokenType.symbol_upTo                  = define( TokenType("..") )
      TokenType.symbol_upToLessThan          = define( TokenType("..<") )
      TokenType.symbol_vertical_bar          = define( TokenType("|") )
      TokenType.symbol_vertical_bar_equals   = define( TokenType("|=") )

    method consume( ch:Character )->Logical
      if (reader.peek != ch) return false
      reader.read
      return true

    method consume( st:String )->Logical
      return reader.consume(st)

    method consume_id( st:String )->Logical
      return reader.consume_id(st)

    method consume_spaces->Logical
      if (not reader.consume(' ')) return false
      while (reader.consume(' '))  noAction
      return true

    method define( type:TokenType )->TokenType
      TokenType.lookup[type.name] = type
      return type

    method error( message:String )->RogueError
      return RogueError( message, filepath, reader.line, reader.column )

    method get_symbol_token_type->TokenType
      local ch = reader.read

      if (ch == '!')
        if (consume('=')) return TokenType.symbol_ne
        else              return TokenType.symbol_exclamation_point

      elseIf (ch == '$' )
        local token_type : TokenType
        local id = read_identifier
        which (id)
          case "define":   add_new_token( TokenType.directive_define )
          case "include":  add_new_token( TokenType.directive_include )
          case "if":       add_new_token( TokenType.directive_if )
          case "elseIf":   add_new_token( TokenType.directive_elseIf )
          case "else":     add_new_token( TokenType.directive_else )
          case "endIf":    add_new_token( TokenType.directive_endIf )
          others
            add_new_token( TokenType.placeholder_id, "$" + id )
        endWhich
        return null

      elseIf (ch == '%')
        if     (consume('=')) return TokenType.symbol_percent_equals
        else                  return TokenType.symbol_percent


      elseIf (ch == '&' )
        if     (consume('&')) throw error( "Use 'and' instead of '&&'." )
        elseIf (consume('=')) return TokenType.symbol_ampersand_equals
        else                  return TokenType.symbol_ampersand

      elseIf (ch == '(')
        return TokenType.symbol_open_paren

      elseIf (ch == ')')
      return TokenType.symbol_close_paren

      elseIf (ch == '*')
        if     (consume('=')) return TokenType.symbol_times_equals
        else                  return TokenType.symbol_times

      elseIf (ch == '+')
        if     (consume('=')) return TokenType.symbol_plus_equals
        elseIf (consume('+')) return TokenType.symbol_plus_plus
        else                  return TokenType.symbol_plus

      elseIf (ch == ',')
        return TokenType.symbol_comma

      elseIf (ch == '-')
        if     (consume('='))  return TokenType.symbol_minus_equals
        elseIf (consume('-'))  return TokenType.symbol_minus_minus
        elseIf (reader.peek(0) == '>' and reader.peek(1) == '>') return TokenType.symbol_minus
        elseIf (consume('>'))  return TokenType.symbol_arrow
        else                   return TokenType.symbol_minus

      elseIf (ch == '.' )
        if (consume('.'))
          if (consume('.'))
            # ellipsis
            while (consume(' ')) noAction
            if (not consume('\n')) throw error( "End of line expected after '...'." )
            return null 
          elseIf (consume('<'))
            return TokenType.symbol_upToLessThan
          elseIf (consume('>'))
            return TokenType.symbol_downToGreaterThan
          else
            return TokenType.symbol_upTo
          endIf
        elseIf (consume('='))
          return TokenType.symbol_dot_equals
        else
          return TokenType.symbol_dot
        endIf

      elseIf (ch == '/')
        if (consume('/'))
          tokenize_alternate_string
          return null
        elseIf (consume('='))
          return TokenType.symbol_slash_equals
        else
          return TokenType.symbol_slash
        endIf

      elseIf (ch == ':')
        if (consume(':'))
          return TokenType.symbol_colon_colon
        endIf

        if (consume("<<:"))  return TokenType.symbol_shift_left
        elseIf (consume(">>:"))  return TokenType.symbol_shift_right
        elseIf (consume(">>>:")) return TokenType.symbol_shift_right_x
        return TokenType.symbol_colon

      elseIf (ch == ';')
        return TokenType.symbol_semicolon

      elseIf (ch == '<' )
        if     (consume('<')) return TokenType.symbol_open_specialize
        elseIf (consume('=')) return TokenType.symbol_le
        elseIf (consume('>')) return TokenType.symbol_compare
        else                  return TokenType.symbol_lt

      elseIf (ch == '=' )
        if (consume('='))     return TokenType.symbol_eq
        elseIf (consume('>')) return TokenType.symbol_fat_arrow
        else                  return TokenType.symbol_equals

      elseIf (ch == '>' )
        if      (consume('=')) return TokenType.symbol_ge
        elseIf  (consume('>')) return TokenType.symbol_close_specialize
        else                   return TokenType.symbol_gt

      elseIf (ch == '?')
        return TokenType.symbol_question_mark

      elseIf (ch == '@' )
        return TokenType.symbol_at

      elseIf (ch == '[')
        if (consume(']')) return TokenType.symbol_empty_brackets
        return TokenType.symbol_open_bracket
        #return check_for_directives( TokenType.symbol_open_bracket )

      elseIf (ch == '\\')
        return TokenType.symbol_backslash

      elseIf (ch == ']')
        return TokenType.symbol_close_bracket

      elseIf (ch == '^')
        if     (consume('=')) return TokenType.symbol_caret_equals
        else                  return TokenType.symbol_caret

      elseIf (ch == '{')
        if (consume('}')) return TokenType.symbol_empty_braces
        return TokenType.symbol_open_brace

      elseIf (ch == '|' )
        if     (consume('|')) throw error( "Use 'or' instead of '||'." )
        elseIf (consume('=')) return TokenType.symbol_vertical_bar_equals
        else                  return TokenType.symbol_vertical_bar

      elseIf (ch == '}' )
        if     (consume('#')) return TokenType.symbol_close_comment # used for error reporting
        else                  return TokenType.symbol_close_brace

      elseIf (ch == '~')
        if     (consume('=')) return TokenType.symbol_tilde_equals
        else                  return TokenType.symbol_tilde

      else
        throw error( "Unexpected input '"+ch+"'." )
      endIf


    method next_is_hex_digit->Logical
      local ch = reader.peek
      return (ch >= '0' and ch <= '9') or (ch >= 'a' and ch <= 'f') or (ch >= 'A' and ch <= 'F')

    method read_character->Character
      if (not reader.has_another) throw error( "Character expected." )

      local ch = reader.peek
      if (ch == '\n') throw error( "Character expected; found end of line." )

      if (ch == '\\')
        reader.read
        if (not reader.has_another) throw error( "Escaped character expected; found end of input." )

        if (consume('b')) return 8->Character
        if (consume('f')) return 12->Character
        if (consume('n')) return '\n'
        if (consume('r')) return '\r'
        if (consume('t')) return '\t'
        if (consume('v')) return 11->Character
        if (consume('0')) return '\0'
        if (consume('/')) return '/'
        if (consume('\''))return '\''
        if (consume('\\'))return '\\'
        if (consume('"')) return '"'
        if (consume('x')) return read_hex_value(2)
        if (consume('u')) return read_hex_value(4)
        throw error( "Invalid escape sequence.  Supported: \\n \\r \\t \\0 \\/ \\' \\\\ \\\" \\" + "uXXXX \\" + "xXX." )
      endIf

      local value = reader.read->Integer
      if ((value & 0x80) != 0)
        # Handle UTF8 encoding
        local ch2 = reader.read->Integer

        if ((value & 0x20) == 0)
          # %110xxxxx 10xxxxxx
          value = value & 0x1f
          ch2 = value & 0x3f
          return ((value:<<:6) | ch2)->Character
        else
          # %1110xxxx 10xxxxxx 10xxxxxx
          local ch3 = reader.read->Integer
          value = value & 15
          ch2 = ch2 & 0x3f
          ch3 = ch3 & 0x3f
          return ((value:<<:2) | (ch2:<<:6) | ch3)->Character
        endIf
      endIf
      return value->Character

    method read_hex_value( digits:Integer )->Character
      local value = 0
      local i = 1
      while (i <= digits)
        if (not reader.has_another) throw error( digits + "-digit hex value expected; found end of file." )
        if (not next_is_hex_digit)
          local ch = reader.peek
          local error_buffer = StringBuilder()
          error_buffer.print( "Invalid hex digit " )
          if (ch < ' ' or ch->Integer == 127) error_buffer.print( ch->Integer )
          else error_buffer.print( "'" + ch + "'" )
          error_buffer.print('.')
          throw error( error_buffer->String )
        endIf
        local intval = reader.read->Integer
        value = (value:<<:4) + intval
        ++i
      endWhile
      return value->Character

    method read_identifier->String
      buffer.clear
      local ch = reader.peek
      while ((ch>='a' and ch<='z') or (ch>='A' and ch<='Z') or (ch>='0' and ch<='9') or ch=='_')
        buffer.print( reader.read )
        ch = reader.peek
      endWhile

      if (buffer.count == 0) throw error( "Identifier expected." )

      return buffer->String

    method tokenize_alternate_string->Logical
      buffer.clear

      while (reader.has_another)
        if (reader.has_another)
          local ch = reader.peek
          if (ch == '/')
            reader.read
            ch = reader.peek
            if (ch == '/')
              reader.read
              return add_new_string_or_character_token_from_buffer('/')
            else
              buffer.print( '/' )
            endIf
          else
            buffer.print( read_character )
          endIf
        endIf
      endWhile

      throw error( "End of file reached while looking for end of string." )

    method tokenize_another->Logical
      reader.consume_spaces

      # Must be the has_another test for the terminating EOL
      next_filepath = filepath
      next_line = reader.line
      next_column = reader.column

      if (not reader.has_another) return false

      local ch = reader.peek
      if (ch == '\n') reader.read; return add_new_token( TokenType.eol )

      if (ch.is_letter or ch == '_')
        local id = read_identifier
        local keyword_type = TokenType.lookup[id]
        if (keyword_type?) return add_new_token( keyword_type )
        else               return add_new_token( TokenType.identifier, id )
        return true

      elseIf (ch == '\'')
        return tokenize_string( '\'' )

      elseIf (ch == '"')
        return tokenize_string( '"' )

      elseIf (ch == '#')
        return tokenize_comment

      elseIf (ch >= '0' and ch <= '9')
        which (reader.peek(1))
          case 'b': return tokenize_integer_in_base(2)
          case 'c': return tokenize_integer_in_base(8)
          case 'x': return tokenize_integer_in_base(16)
          others:   return tokenize_number
        endWhich

      elseIf (ch == '@' and reader.peek(1) == '|')
        return tokenize_verbatim_string

      else
        # Either a symbol or a decimal like ".1".
        if (ch == '.')
          local next = reader.peek(1)
          if (next >= '0' and next <= '9') return tokenize_number
        endIf

        # Symbol
        local token_type = get_symbol_token_type
        if (token_type is null) return true
        return add_new_token( token_type )
      endIf

      local name = "'$'" (ch)
      if (ch == 10) name = "EOL"
      elseIf (ch < 32 or ch > 126) name = "(Unicode $)" (ch->Integer)

      throw error( "Syntax error - unexpected input $." (name) )

    method tokenize_comment->Logical
      buffer.clear
      reader.read  # '#'
      if (consume('{'))
        local nesting_count = 1
        local keep_reading = true
        while (reader.has_another)
          local ch = reader.read
          which (ch)
            case '#'
              buffer.print('#')
              if (consume('{'))
                buffer.print('{')
                ++nesting_count
              endIf

            case '}'
              if (consume('#'))
                --nesting_count
                if (nesting_count == 0) escapeWhile
                else buffer.print('}'); buffer.print('#')
              else
                buffer.print('}')
              endIf

            others
              buffer.print( ch )
          endWhich
        endWhile
      else
        while (reader.has_another and reader.peek != '\n') buffer.print( reader.read )
      endIf

      if (tokens.count? and tokens.last.type is TokenType.eol)
        # Store comments in preceding EOL, if any.
        (tokens.last as EOLToken).comment += buffer->String
      endIf

      return true

    method tokenize_integer_in_base( base:Integer )->Logical
      reader.read  # '0'
      reader.read  # [b,c,x] = [2,8,16]

      local bits_per_value = 1
      while (2^bits_per_value < base) ++bits_per_value

      local count = 0
      local n = 0
      local digit = reader.peek.to_number
      while (reader.has_another and digit != -1)
        if (digit >= base) throw error( "Digit out of range for base " + base + "." )
        ++count
        n = n * base + digit
        reader.read
        digit = reader.peek.to_number
      endWhile

      if (count == 0) throw error( "One or more digits expected." )

      if (bits_per_value * count > 32) throw error( "Number too large for base " + base + "." )

      return add_new_token( TokenType.literal_integer, n )

    method tokenize_number->Logical
      local is_negative = consume('-')
      local is_Real = false

      local n  = tokenize_Integer

      if (reader.peek == '.')
        local ch = reader.peek(1)
        if (ch >= '0' and ch <= '9')
          reader.read
          is_Real = true
          local start_pos = reader.position
          local fraction = tokenize_Integer
          n += fraction / 10^(reader.position - start_pos)
        elseIf (ch == '.')
          # Start of range
          if (is_negative) n = -n
          return add_new_token( TokenType.literal_integer, n )
        else
          if (is_negative) n = -n
          return add_new_token( TokenType.literal_real, n )
        endIf
      endIf

      if (consume('E') or consume('e'))
        is_Real = true
        local negative_exponent = consume('-')
        if (not negative_exponent) consume('+')
        local power = tokenize_Integer
        if (negative_exponent) n /= power
        else                   n *= power
      endIf

      if (is_negative) n = -n;

      if (is_Real)
        return add_new_token( TokenType.literal_real, n )
      else
        return add_new_token( TokenType.literal_integer, n->Integer )
      endIf

    method tokenize_Integer->Real
      local n = 0.0
      local ch = reader.peek
      while (ch >= '0' and ch <= '9')
        local intval = reader.read->Integer - '0'
        n = n * 10.0 + intval
        ch = reader.peek
      endWhile
      return n

    method tokenize_string( terminator:Character )->Logical
      buffer.clear
      reader.read
      while (reader.has_another)
        local ch = reader.peek
        if (ch == terminator)
          reader.read
          return add_new_string_or_character_token_from_buffer(terminator)
        else
          buffer.print( read_character )
        endIf
      endWhile

      throw error( "End of input reached while looking for end of string." )

    method tokenize_verbatim_string->Logical
      buffer.clear
      reader.read
      reader.read
      while (reader.has_another)
        local ch = reader.peek
        if (ch == 10)
          local eol_line = reader.line
          local eol_column = reader.column
          reader.read
          consume_spaces
          if (consume('|'))
            buffer.print( ch )
          else
            add_new_token( TokenType.literal_string, buffer->String )
            return add_new_token( TokenType.eol )
          endIf
        else
          buffer.print( reader.read )
        endIf
      endWhile

      throw error( "End of File reached while looking for end of verbatim string." )
endClass

