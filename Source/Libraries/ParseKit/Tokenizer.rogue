module ParseKit

class Tokenizer<<$ID>>
  GLOBAL PROPERTIES
    keywords  : StringTable<<TokenType<<$ID>>>>
    symbols   : Table<<Character,TokenType<<$ID>>[]>>

  PROPERTIES
    filepath       : String
    scanner        : Scanner
    tokens         : Token[]
    spaces_per_tab = 2
    keep_eols      = true
    keep_spaces    = false

  METHODS
    method init_object
      if (not keywords)
        configure_token_types( <<$ID>> )
      endIf

    method configure_token_types( type_info:TypeInfo )
      ensure keywords
      ensure symbols

      local base_type_info = type_info.base_class
      if (base_type_info) configure_token_types( base_type_info )

      forEach (p in type_info.global_properties)
        if (p.type.instance_of(<<TokenType<<$ID>>>>))
          configure_token_type( type_info.global_property( p )->Object as TokenType<<$ID>> )
        endIf
      endForEach

    method configure_token_type( type:TokenType<<$ID>> )
      if (type.is_keyword)
        keywords[ type.name ] = type
      elseIf (type.is_symbol)
        local first_ch = type.name[ 0 ]
        local list = symbols[ first_ch ]
        if (not list) symbols[ first_ch ] = ensure list
        if (list.add(type).count > 1) list.sort( (a,b)=>(a.name.count>=b.name.count) )
      endIf

    method consume( ch:Character )->Logical [macro]
      this.scanner.consume( ch )

    method consume( text:String )->Logical [macro]
      this.scanner.consume( text )

    method consume_eols->Logical
      local found_any = false
      while (consume('\n')) found_any = true
      return found_any

    method consume_spaces->Logical
      local found_any = false
      while (consume(' ')) found_any = true
      return found_any

    method consume_whitespace->Logical
      local found_any = false
      while ((not keep_spaces and consume_spaces) or (not keep_eols and consume_eols)) found_any = true
      return true

    method peek->Character [macro]
      this.scanner.peek

    method read->Character [macro]
      this.scanner.read

    method tokenize( filepath )->Token[]
      scanner = Scanner( File(filepath), &=spaces_per_tab )
      return tokenize

    method tokenize( filepath, source:String )->Token[]
      scanner = Scanner( source, &=spaces_per_tab )
      return tokenize

    method tokenize( filepath, source:Character[] )->Token[]
      scanner = Scanner( source, &=spaces_per_tab )
      return tokenize

    method tokenize->Token[]
      tokens = Token[]
      while (tokenize_another) noAction
      tokens.add( Token(TokenType<<$ID>>.EOL) )
      return tokens

    method tokenize_another->Logical
      Token.next_filepath = filepath
      Token.next_line = scanner.line
      Token.next_column = scanner.column

      consume_whitespace
      if (not scanner.has_another) return false

      if (consume(' '))  tokens.add( Token(TokenType<<$ID>>.SPACE) ); return true
      if (consume('\n')) tokens.add( Token(TokenType<<$ID>>.EOL) ); return true

      if (tokenize_comment) return true
      if (tokenize_number)  return true
      if (tokenize_symbol)  return true

      throw ParseError( filepath, scanner.line, "Syntax error: unexpected '$'." (peek) )

    method tokenize_comment->Logical
      local text = scan_comment
      if (not text) return false

      if (tokens.count and tokens.last.type is TokenType<<$ID>>.EOL)
        if (not tokens.last.text) tokens.last.text = text
        else                      tokens.last.text += text
      endIf
      return true

    method tokenize_number->Logical
      if (not peek.is_number) return false

      local base = 10
      if (consume("0b"))     base = 2
      if (consume("0c"))     base = 8
      elseIf (consume("0x")) base = 16

      local n = scan_int64( base )
      #if (base == 10)
      if (n >= Int32.minimum and n <= Int32.maximum)
        tokens.add( Token(TokenType<<$ID>>.LITERAL_INT32,n) )
      else
        tokens.add( Token(TokenType<<$ID>>.LITERAL_INT64,n.real_bits) )
      endIf

      return true

    method tokenize_symbol->Logical
      local candidates = symbols[ peek ]
      if (not candidates) return false

      # Candidates are ordered from most characters to least - the first one to fully match is the best choice
      forEach (candidate in candidates)
        if (scanner.consume(candidate.name))
          tokens.add( Token(candidate) )
          return true
        endIf
      endForEach

      return false

    method scan_comment->String
      if (not consume('#')) return null

      use buffer = StringBuilder.pool
        if (consume('{'))
          # Multi-line comment
          while (scanner.has_another and not consume("}#")) buffer.print( read )
        else
          # Single-line comment
          while (scanner.has_another and not peek == '\n')
            buffer.print( read )
          endWhile
        endIf
        return buffer->String
      endUse

    method scan_int64( base:Int32 )->Int64
      local result : Int64
      while (peek.is_number(base))
        result = result * base + read.to_number
      endWhile
      return result


endClass

