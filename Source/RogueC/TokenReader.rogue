class TokenReader : Reader<<Token>>
  PROPERTIES
    tokens   : Token[]
    position : Int32
    count    : Int32

  METHODS
    method init( tokens )
      count = tokens.count

    method consume( type:TokenType )->Logical
      if (not next_is(type)) return false
      read
      return true

    method error( message:String )->Error
      if (has_another) return peek.error( message )
      if (count) return tokens.last.error( message )
      return RogueError( message )

    method has_another->Logical
      return (position < count)

    method must_consume( type:TokenType )
      if (not consume(type)) throw peek.error( "Unexpected $." (peek.quoted_name) )

    method next_is( type:TokenType )->Logical
      if (position == count) return false
      return tokens[position].type is type

    method next_is_statement_token->Logical
      if (position == count) return false
      if (tokens[position].type.is_structure) return false
      return true

    method peek->Token
      if (position == count) return TokenType.eoi.create_token( tokens.last )
      return tokens[position]

    method peek( num_ahead:Int32 )->Token
      if (position + num_ahead >= count) return TokenType.eoi.create_token( tokens.last )
      return tokens[ position + num_ahead ]

    method read->Token
      ++position
      return tokens[ position - 1 ]

endClass


class PreprocessorTokenReader : Reader<<Token>>
  # Expands tokens based on Preprocessor.definitions
  PROPERTIES
    tokens   : Token[]
    stack    = Token[]  # stored in reverse order
    position : Int32
    count    : Int32
    unfiltered : Int32

    local_definitions = Definitions[]

  METHODS
    method init( tokens )
      count = tokens.count

    method error( message:String )->Error
      if (has_another) return peek.error( message )
      if (count) return TokenType.eoi.create_token( tokens.last ).error( message )
      return RogueError( message )

    method expand_definition( t:Token )
      if (local_definitions.count and expand_definition(t,local_definitions.last))  return
      if (expand_definition(t,Preprocessor.definitions)) return
      stack.add( t ) # no expansion

    method expand_definition( t:Token, definitions:Definitions )->Logical
      local defs = definitions.defines_for_name[ t->String ]
      if (not defs) return false

      local max_args = 0
      local zero_param_define : Define
      forEach (def in defs)
        local n = def.parameter_count
        max_args = max_args.or_larger( n )
        if (n == 0) zero_param_define = def
      endForEach

      if (max_args > 0)
        # Collect arg tokens
        if (peek.type is TokenType.symbol_open_paren)
          local args = Token[][]
          local all_tokens = Token[]
          all_tokens.add( read )  # '('
          if (peek.type is TokenType.symbol_close_paren)
            all_tokens.add( read ) # ')'
          else
            local tokens = Token[]
            args.add( tokens )
            local depth = 1
            while (has_another)
              local macro_t = read
              all_tokens.add( macro_t )

              if (macro_t.type is TokenType.symbol_open_paren or macro_t.type is TokenType.symbol_open_bracket or...
                macro_t.type is TokenType.symbol_open_brace or macro_t.type is TokenType.symbol_open_specialize)
                if (depth) tokens.add( macro_t )
                ++depth
              elseIf (macro_t.type is TokenType.symbol_close_paren or macro_t.type is TokenType.symbol_close_bracket or...
                macro_t.type is TokenType.symbol_close_brace or macro_t.type is TokenType.symbol_close_specialize)
                --depth
                if (depth == 0) escapeWhile
                else            tokens.add( macro_t )
              elseIf (macro_t.type is TokenType.symbol_comma and depth == 1)
                tokens = Token[]
                args.add( tokens )
              else
                tokens.add( macro_t )
              endIf
            endWhile

            if (depth) throw peek.error( "Unexpected end of input." )
          endIf

          local def = defs.find( args.count )
          if (def)
            apply_macro( t, def, args )
            return true
          else
            push( all_tokens )
            if (not zero_param_define)
              local builder = StringBuilder()
              builder.println( "No '$' macro accepts $. Candidates:\n" (t->String,"# arg".pluralized(args.count)) )
              forEach (candidate in defs)
                builder.println "  $" (candidate)
              endForEach
              builder.println
              throw t.error( builder->String )
            endIf
          endIf
        endIf
      endIf

      if (zero_param_define)
        apply_define( t, zero_param_define )
        return true
      else
        return false
      endIf

    method apply_define( t:Token, def:Define )
      if (def.is_in_use) throw t.error( ''Recursive definition of "$".'' (t->String) )
      def.is_in_use = true # prevent infinite recursion

      forEach (def_t in def.tokens step -1)
        def_t = def_t.cloned.set_location( t )
        if (def_t.type is TokenType.identifier)
          expand_definition( def_t )
        else
          stack.add( def_t )
        endIf
      endForEach

      def.is_in_use = false

    method apply_macro( t:Token, def:Define, args:Token[][] )
      if (def.is_in_use) throw t.error( ''Recursive definition of "$".'' (t->String) )
      def.is_in_use = true

      local lookup = StringTable<<Token[]>>()
      forEach (param_name at index in def.parameter_names)
        lookup[ param_name ] = args[ index ]
      endForEach

      forEach (def_t in def.tokens step -1)
        def_t = def_t.cloned.set_location( t )
        if (def_t.type is TokenType.identifier)
          local arg_tokens = lookup[ def_t->String ]
          if (arg_tokens)
            forEach (arg_t in arg_tokens step -1)
              arg_t = arg_t.cloned.set_location( t )
              if (arg_t.type is TokenType.identifier)
                expand_definition( arg_t )
              else
                stack.add( arg_t )
              endIf
            endForEach
          else
            expand_definition( def_t )
          endIf
        else
          stack.add( def_t )
        endIf
      endForEach

      def.is_in_use = false

    method has_another->Logical
      peek
      return stack.count

    method next_is( type:TokenType )->Logical
      if (position == count and stack.count == 0) return false
      return (peek.type is type)

    method next_is_unfiltered( type:TokenType )->Logical
      if (position == count and stack.count == 0) return false
      return (peek_unfiltered.type is type)

    method peek->Token
      if (stack.count) return stack.last
      if (this.unfiltered) return peek_unfiltered

      if (position >= count + stack.count)
        return TokenType.eoi.create_token( tokens.last )
      endIf

      local t = tokens[position]
      ++position

      # Possible definition expansion
      if (t.type is TokenType.identifier)
        expand_definition( t )
      else
        stack.add( t )
      endIf

      return stack.last

    method peek_unfiltered->Token
      if (stack.count) return stack.last

      if (position >= count + stack.count)
        return TokenType.eoi.create_token( tokens.last )
      endIf

      stack.add( tokens[position] )
      ++position

      return stack.last

    method push_unfiltered()
      unfiltered++

    method pop_unfiltered()
      unfiltered--

    method peek( num_ahead:Int32 )->Token
      which (num_ahead)
        case 0
          return peek
        case 1
          local t1 = read
          local t2 = peek
          push( t1 )
          return t2
        case 2
          local t1 = read
          local t2 = read
          local t3 = peek
          push( t2 )
          push( t1 )
          return t3
        others
          local tokens = Token[]
          loop (num_ahead-1) tokens.add( read )
          local result = peek
          push( tokens )
          return result
      endWhich

    method push( t:Token )
      stack.add( t )

    method push( _tokens:Token[] )
      local i = _tokens.count
      while (i > 0)
        --i
        stack.add( _tokens[i] )
      endWhile

    method read->Token
      peek
      if (stack.count == 0) throw error( "Unexpected end of input." )
      return stack.remove_last

    method read_identifier->String
      if (not next_is(TokenType.identifier))
        throw error( "Identifier expected, found $." (peek.quoted_name) )
      endIf
      return read->String

    method read_identifier_unfiltered->String
      if (not next_is_unfiltered(TokenType.identifier))
        throw error( "Identifier expected, found $." (peek_unfiltered.quoted_name) )
      endIf
      return read->String

endClass

