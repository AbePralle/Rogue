class TokenReader
  PROPERTIES
    tokens   : Token[]
    position : Int32
    count    : Int32

  METHODS
    method init( tokens )
      count = tokens.count

    method error( message:String )->Error
      if (has_another) return peek.error( message )
      if (count) return tokens.last.error( message )
      return RogueError( message )

    method has_another->Logical
      return (position < count)

    method next_is( type:TokenType )->Logical
      if (position == count) return false
      return tokens[position].type is type

    method next_is_statement_token->Logical
      if (position == count) return false
      if (tokens[position].type.is_structure) return false
      return true

    method peek->Token
      if (position == count) return TokenType.eoi.create_token( tokens.last )
      return tokens[position]

    method peek( num_ahead:Int32 )->Token
      if (position + num_ahead >= count) return TokenType.eoi.create_token( tokens.last )
      return tokens[ position + num_ahead ]

    method read->Token
      ++position
      return tokens[ position - 1 ]

endClass


class PreprocessorTokenReader
  # Expands tokens based on Preprocessor.definitions
  PROPERTIES
    tokens   : Token[]
    stack    = Token[]  # stored in reverse order
    position : Int32
    count    : Int32
    unfiltered : Int32

    local_definitions = Definitions[]

  METHODS
    method init( tokens )
      count = tokens.count

    method error( message:String )->Error
      if (has_another) return peek.error( message )
      if (count) return TokenType.eoi.create_token( tokens.last ).error( message )
      return RogueError( message )

    method expand_definition( t:Token )
      if (local_definitions.count and expand_definition(t,local_definitions.last))  return
      if (expand_definition(t,Preprocessor.definitions)) return
      stack.add( t ) # no expansion

    method expand_definition( t:Token, definitions:Definitions )->Logical
      local defs = definitions.defines_for_name[ t->String ]
      if (not defs) return false

      local max_args = 0
      local zero_param_define : Define
      forEach (def in defs)
        local n = def.parameter_count
        max_args = max_args.or_larger( n )
        if (n == 0) zero_param_define = def
      endForEach

      if (max_args > 0)
        # Collect arg tokens
        if (peek.type is TokenType.symbol_open_paren)
          local args = Token[][]
          local all_tokens = Token[]
          all_tokens.add( read )  # '('
          if (peek.type is TokenType.symbol_close_paren)
            all_tokens.add( read ) # ')'
          else
            local tokens = Token[]
            args.add( tokens )
            local depth = 1
            while (has_another)
              local macro_t = read
              all_tokens.add( macro_t )

              if (macro_t.type is TokenType.symbol_open_paren or macro_t.type is TokenType.symbol_open_bracket or...
                macro_t.type is TokenType.symbol_open_brace or macro_t.type is TokenType.symbol_open_specialize)
                if (depth) tokens.add( macro_t )
                ++depth
              elseIf (macro_t.type is TokenType.symbol_close_paren or macro_t.type is TokenType.symbol_close_bracket or...
                macro_t.type is TokenType.symbol_close_brace or macro_t.type is TokenType.symbol_open_specialize)
                --depth
                if (depth == 0) escapeWhile
                else            tokens.add( macro_t )
              elseIf (macro_t.type is TokenType.symbol_comma and depth == 1)
                tokens = Token[]
                args.add( tokens )
              else
                tokens.add( macro_t )
              endIf
            endWhile

            if (depth) throw peek.error( "Unexpected end of input." )
          endIf

          local def = defs.find( args.count )
          if (def)
            apply_macro( t, def, args )
            return true
          else
            push( all_tokens )
            if (not zero_param_define)
              local builder = StringBuilder()
              builder.println( "No '$' macro accepts $. Candidates:\n" (t->String,"# arg".pluralized(args.count)) )
              forEach (candidate in defs)
                builder.println "  $" (candidate)
              endForEach
              builder.println
              throw t.error( builder->String )
            endIf
          endIf
        endIf
      endIf

      if (zero_param_define)
        apply_define( t, zero_param_define )
        return true
      else
        return false
      endIf

    method apply_define( t:Token, def:Define )
      if (def.is_in_use) throw t.error( ''Recursive definition for "$".'' (t->String) )
      def.is_in_use = true # prevent infinite recursion

      forEach (def_t in def.tokens step -1)
        if (def_t.type is TokenType.identifier)
          expand_definition( def_t )
        else
          stack.add( def_t )
        endIf
      endForEach

      def.is_in_use = false

    method apply_macro( t:Token, def:Define, args:Token[][] )
      if (def.is_in_use) throw t.error( ''Recursive definition for "$".'' (t->String) )
      def.is_in_use = true

      local lookup = StringTable<<Token[]>>()
      forEach (param_name at index in def.parameter_names)
        lookup[ param_name ] = args[ index ]
      endForEach

      forEach (def_t in def.tokens step -1)
        if (def_t.type is TokenType.identifier)
          local arg_tokens = lookup[ def_t->String ]
          if (arg_tokens)
            forEach (arg_t in arg_tokens step -1)
              if (arg_t.type is TokenType.identifier)
                expand_definition( arg_t )
              else
                stack.add( arg_t )
              endIf
            endForEach
          else
            expand_definition( def_t )
          endIf
        else
          stack.add( def_t )
        endIf
      endForEach

      def.is_in_use = false

    method has_another->Logical
      peek
      return stack.count

    method next_is( type:TokenType )->Logical
      if (position == count and stack.count == 0) return false
      return (peek.type is type)

    method next_is_unfiltered( type:TokenType )->Logical
      if (position == count and stack.count == 0) return false
      return (peek_unfiltered.type is type)

    method peek->Token
      if (stack.count) return stack.last
      return peek(0)

    method peek_unfiltered->Token
      if (stack.count) return stack.last
      return peek_unfiltered(0)

    method push_unfiltered()
      unfiltered++

    method pop_unfiltered()
      unfiltered--

    method peek( num_ahead:Int32, unfiltered:Logical )->Token
      if (position + num_ahead >= count + stack.count)
        return TokenType.eoi.create_token( tokens.last )
      endIf

      while (stack.count <= num_ahead)
        local t = tokens[position]
        ++position

        # Possible definition expansion
        if (t.type is TokenType.identifier and unfiltered == false and this.unfiltered == 0)
          expand_definition( t )
        else
          stack.add( t )
        endIf
      endWhile

      return stack[ (stack.count - num_ahead) - 1 ]

    method peek( num_ahead:Int32 )->Token
      return peek(num_ahead, false)

    method peek_unfiltered( num_ahead:Int32 )->Token
      return peek(num_ahead, true)

    method push( t:Token )
      stack.add( t )

    method push( _tokens:Token[] )
      local i = _tokens.count
      while (i > 0)
        --i
        stack.add( _tokens[i] )
      endWhile

    method read->Token
      peek
      if (stack.count == 0) throw error( "Unexpected end of input." )
      return stack.remove_last

    method read_identifier->String
      if (not next_is(TokenType.identifier))
        throw error( "Identifier expected, found $." (peek.quoted_name) )
      endIf
      return read->String

    method read_identifier_unfiltered->String
      if (not next_is_unfiltered(TokenType.identifier))
        throw error( "Identifier expected, found $." (peek_unfiltered.quoted_name) )
      endIf
      return read->String

      #{
    method source_string( i1:Int32, i2:Int32 )->String
      local buffer = StringBuilder()
      local i = i1
      while (i <= i2)
        local lhs_is_letter = (buffer.count and buffer.last.is_letter)
        local token_as_string = tokens[i].to_source_string
        local rhs_is_letter = token_as_string.count and token_as_string[0].is_letter
        if (lhs_is_letter and rhs_is_letter) buffer.print(' ')
        buffer.print( tokens[i].to_source_string )
        ++i
      endWhile
      return buffer.to_String
      }#
endClass

