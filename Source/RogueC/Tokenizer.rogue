class Tokenizer
  PROPERTIES
    filepath    : String
    reader      : ParseReader
    tokens      = Token[]
    buffer      = StringBuilder()

    next_filepath : String
    next_line     : Integer
    next_column   : Integer

  METHODS
    method tokenize( filepath )->Token[]
      return tokenize( ParseReader(File(filepath)) )

    method tokenize( reference_t:Token, filepath, data:String )->Token[]
      local characters = Character[]( data.count )
      forEach (ch in data) characters.add( ch )
      return tokenize( ParseReader(characters).set_position(reference_t.line,reference_t.column) )

    method tokenize( reader )->Token[]
      configure_token_types
      while (tokenize_another) noAction

      if (tokens.count == 0)
        # Ensure there's an EOL token at the end.
        if (tokens.count == 0 or tokens.last.type isNot TokenType.eol)
          add_new_token( TokenType.eol )
        endIf
      endIf

      return tokens

    # -------------------------------------------------------------------------

    method add_new_string_or_character_token_from_buffer( terminator:Character )->Logical
      # We have a string in 'buffer' already; convert it to a character if it has count 1.
      if (buffer.count == 1 and terminator == '\'')
        return add_new_token( TokenType.literal_character, buffer[0] )
      else
        return add_new_token( TokenType.literal_string, buffer->String )
      endIf

    method add_new_token( type:TokenType )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column) )
      return true

    method add_new_token( type:TokenType, value:Character )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Long )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Integer )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Real )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:String )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method configure_token_types
      if (TokenType.lookup) return

      TokenType.lookup        = Table<<String,TokenType>>()

      # Directives
      TokenType.directive_define              = define( DirectiveTokenType("$define") )
      TokenType.directive_include             = define( DirectiveTokenType("$include") )
      TokenType.directive_includeNativeCode   = define( DirectiveTokenType("$includeNativeCode") )
      TokenType.directive_includeNativeHeader = define( DirectiveTokenType("$includeNativeHeader") )
      TokenType.directive_if                  = define( DirectiveTokenType("$if") )
      TokenType.directive_elseIf              = define( DirectiveTokenType("$elseIf") )
      TokenType.directive_else                = define( DirectiveTokenType("$else") )
      TokenType.directive_endIf               = define( DirectiveTokenType("$endIf") )
      TokenType.directive_requisite           = define( StructuralDirectiveTokenType("$requisite") )

      TokenType.placeholder_id       = define( TokenType("$id") )

      # Structure Tokens
      TokenType.eol                   = define( EOLTokenType("end of line") )
      TokenType.begin_augment_tokens  = define( StructureTokenType("end of line") )
      TokenType.keyword_augment       = define( StructureTokenType("augment") )
      TokenType.keyword_case          = define( StructureTokenType("case") )
      TokenType.keyword_catch         = define( StructureTokenType("catch") )
      TokenType.keyword_class         = define( StructureTokenType("class") )
      TokenType.keyword_DEFINITIONS   = define( StructureTokenType("DEFINITIONS") )
      TokenType.keyword_else          = define( StructureTokenType("else") )
      TokenType.keyword_elseIf        = define( StructureTokenType("elseIf") )
      TokenType.keyword_endAugment    = define( StructureTokenType("endAugment") )
      TokenType.keyword_endClass      = define( StructureTokenType("endClass") )
      TokenType.keyword_endContingent = define( StructureTokenType("endContingent") )
      TokenType.keyword_endForEach    = define( StructureTokenType("endForEach") )
      TokenType.keyword_endFunction   = define( StructureTokenType("endFunction") )
      TokenType.keyword_endIf         = define( StructureTokenType("endIf") )
      TokenType.keyword_endLoop       = define( StructureTokenType("endLoop") )
      TokenType.keyword_endTry        = define( StructureTokenType("endTry") )
      TokenType.keyword_endWhich      = define( StructureTokenType("endWhich") )
      TokenType.keyword_endWhile      = define( StructureTokenType("endWhile") )
      TokenType.keyword_ENUMERATE     = define( StructureTokenType("ENUMERATE") )
      TokenType.keyword_GLOBALS       = define( StructureTokenType("GLOBALS") )
      TokenType.keyword_macro         = define( StructureTokenType("macro") )
      TokenType.keyword_method        = define( StructureTokenType("method") )
      TokenType.keyword_METHODS       = define( StructureTokenType("METHODS") )
      TokenType.keyword_nativeCode    = define( StructureTokenType("nativeCode") )
      TokenType.keyword_nativeHeader  = define( StructureTokenType("nativeHeader") )
      TokenType.keyword_others        = define( StructureTokenType("others") )
      TokenType.keyword_PROPERTIES    = define( StructureTokenType("PROPERTIES") )
      TokenType.keyword_routine       = define( StructureTokenType("routine") )
      TokenType.keyword_ROUTINES      = define( StructureTokenType("ROUTINES") )
      TokenType.keyword_satisfied     = define( StructureTokenType("satisfied") )
      TokenType.keyword_unsatisfied   = define( StructureTokenType("unsatisfied") )
      TokenType.keyword_with          = define( StructureTokenType("with") )

      TokenType.symbol_close_brace           = define( StructureTokenType("}") )
      TokenType.symbol_close_bracket         = define( StructureTokenType("]") )
      TokenType.symbol_close_comment         = define( StructureTokenType("}#") )
      TokenType.symbol_close_paren           = define( StructureTokenType(")") )
      TokenType.symbol_close_specialize      = define( StructureTokenType(">>") )

      # Statement Tokens
      TokenType.keyword_await            = define( TokenType("await") )
      TokenType.keyword_contingent       = define( TokenType("contingent") )
      TokenType.keyword_escapeContingent = define( TokenType("escapeContingent") )
      TokenType.keyword_escapeForEach    = define( TokenType("escapeForEach") )
      TokenType.keyword_escapeIf         = define( TokenType("escapeIf") )
      TokenType.keyword_escapeLoop       = define( TokenType("escapeLoop") )
      TokenType.keyword_escapeTry        = define( TokenType("escapeTry") )
      TokenType.keyword_escapeWhich      = define( TokenType("escapeWhich") )
      TokenType.keyword_escapeWhile      = define( TokenType("escapeWhile") )
      TokenType.keyword_forEach          = define( TokenType("forEach") )
      TokenType.keyword_function         = define( TokenType("function") )
      TokenType.keyword_if               = define( TokenType("if") )
      TokenType.keyword_in               = define( TokenType("in") )
      TokenType.keyword_is               = define( TokenType("is") )
      TokenType.keyword_isNot            = define( TokenType("isNot") )
      TokenType.keyword_local            = define( TokenType("local") )
      TokenType.keyword_loop             = define( TokenType("loop") )
      TokenType.keyword_native           = define( TokenType("native") )
      TokenType.keyword_necessary        = define( TokenType("necessary") )
      TokenType.keyword_nextIteration    = define( TokenType("nextIteration") )
      TokenType.keyword_noAction         = define( TokenType("noAction") )
      TokenType.keyword_null             = define( TokenType("null") )
      TokenType.keyword_of               = define( TokenType("of") )
      TokenType.keyword_return           = define( TokenType("return") )
      TokenType.keyword_step             = define( TokenType("step") )
      TokenType.keyword_sufficient       = define( TokenType("sufficient") )
      TokenType.keyword_throw            = define( TokenType("throw") )
      TokenType.keyword_trace            = define( TokenType("trace") )
      TokenType.keyword_try              = define( TokenType("try") )
      TokenType.keyword_which            = define( TokenType("which") )
      TokenType.keyword_while            = define( TokenType("while") )
      TokenType.keyword_yield            = define( TokenType("yield") )

      # Expression Tokens
      TokenType.identifier                   = TokenType("identifier")
      TokenType.type_identifier              = TokenType("type identifier")
      TokenType.literal_character            = TokenType("Character")
      TokenType.literal_integer              = TokenType("Integer")
      TokenType.literal_long                 = TokenType("Long")
      TokenType.literal_real                 = TokenType("Real")
      TokenType.literal_string               = TokenType("String")

      TokenType.keyword_and                  = define( TokenType("and") )
      TokenType.keyword_as                   = define( TokenType("as") )
      TokenType.keyword_false                = define( TokenType("false") )
      TokenType.keyword_instanceOf           = define( TokenType("instanceOf") )
      TokenType.keyword_meta                 = define( TokenType("meta") )
      TokenType.keyword_not                  = define( TokenType("not") )
      TokenType.keyword_notInstanceOf        = define( TokenType("notInstanceOf") )
      TokenType.keyword_or                   = define( TokenType("or") )
      TokenType.keyword_pi                   = define( TokenType("pi") )
      TokenType.keyword_prior                = define( TokenType("prior") )
      TokenType.keyword_this                 = define( TokenType("this") )
      TokenType.keyword_true                 = define( TokenType("true") )
      TokenType.keyword_xor                  = define( TokenType("xor") )

      TokenType.symbol_ampersand             = define( TokenType("&") )
      TokenType.symbol_ampersand_equals      = define( OpWithAssignTokenType("&=") )
      TokenType.symbol_arrow                 = define( TokenType("->") )
      TokenType.symbol_at                    = define( TokenType("@") )
      TokenType.symbol_backslash             = define( TokenType("\\") )
      TokenType.symbol_caret                 = define( TokenType("^") )
      TokenType.symbol_caret_equals          = define( OpWithAssignTokenType("^=") )
      TokenType.symbol_colon                 = define( TokenType(":") )
      TokenType.symbol_colon_colon           = define( TokenType("::") )
      TokenType.symbol_comma                 = define( TokenType(",") )
      TokenType.symbol_compare               = define( TokenType("<>") )
      TokenType.symbol_dot                   = define( TokenType(".") )
      TokenType.symbol_dot_equals            = define( OpWithAssignTokenType(".=") )
      TokenType.symbol_downToGreaterThan     = define( TokenType("..>") )
      TokenType.symbol_empty_braces          = define( TokenType("{}") )
      TokenType.symbol_empty_brackets        = define( TokenType("[]") )
      TokenType.symbol_eq                    = define( OpWithAssignTokenType("==") )
      TokenType.symbol_equals                = define( TokenType("=") )
      TokenType.symbol_exclamation_point     = define( TokenType("!") )
      TokenType.symbol_fat_arrow             = define( TokenType("=>") )
      TokenType.symbol_ge                    = define( OpWithAssignTokenType(">=") )
      TokenType.symbol_gt                    = define( TokenType(">") )
      TokenType.symbol_le                    = define( OpWithAssignTokenType("<=") )
      TokenType.symbol_lt                    = define( TokenType("<") )
      TokenType.symbol_minus                 = define( TokenType("-") )
      TokenType.symbol_minus_equals          = define( OpWithAssignTokenType("-=") )
      TokenType.symbol_minus_minus           = define( TokenType("--") )
      TokenType.symbol_ne                    = define( OpWithAssignTokenType("!=") )
      TokenType.symbol_open_brace            = define( TokenType("{") )
      TokenType.symbol_open_bracket          = define( TokenType("[") )
      TokenType.symbol_open_paren            = define( TokenType("(") )
      TokenType.symbol_open_specialize       = define( TokenType("<<") )
      TokenType.symbol_percent               = define( TokenType("%") )
      TokenType.symbol_percent_equals        = define( OpWithAssignTokenType("%=") )
      TokenType.symbol_plus                  = define( TokenType("+") )
      TokenType.symbol_plus_equals           = define( OpWithAssignTokenType("+=") )
      TokenType.symbol_plus_plus             = define( TokenType("++") )
      TokenType.symbol_question_mark         = define( TokenType("?") )
      TokenType.symbol_semicolon             = define( TokenType(";") )
      TokenType.symbol_shift_left            = define( TokenType(":<<:") )
      TokenType.symbol_shift_right           = define( TokenType(":>>:") )
      TokenType.symbol_shift_right_x         = define( TokenType(":>>>:") )
      TokenType.symbol_slash                 = define( TokenType("/") )
      TokenType.symbol_slash_equals          = define( OpWithAssignTokenType("/=") )
      TokenType.symbol_tilde                 = define( TokenType("~") )
      TokenType.symbol_tilde_equals          = define( OpWithAssignTokenType("~=") )
      TokenType.symbol_times                 = define( TokenType("*") )
      TokenType.symbol_times_equals          = define( OpWithAssignTokenType("*=") )
      TokenType.symbol_upTo                  = define( TokenType("..") )
      TokenType.symbol_upToLessThan          = define( TokenType("..<") )
      TokenType.symbol_vertical_bar          = define( TokenType("|") )
      TokenType.symbol_vertical_bar_equals   = define( OpWithAssignTokenType("|=") )
      TokenType.symbol_double_vertical_bar   = define( TokenType("||") )

    method consume( ch:Character )->Logical
      if (reader.peek != ch) return false
      reader.read
      return true

    method consume( st:String )->Logical
      return reader.consume(st)

    method consume_id( st:String )->Logical
      return reader.consume_id(st)

    method consume_spaces->Logical
      if (not reader.consume(' ')) return false
      while (reader.consume(' '))  noAction
      return true

    method define( type:TokenType )->TokenType
      TokenType.lookup[type.name] = type
      return type

    method error( message:String )->RogueError
      return RogueError( message, filepath, reader.line, reader.column )

    method get_symbol_token_type->TokenType
      local ch = reader.read

      if (ch == '!')
        if (consume('=')) return TokenType.symbol_ne
        else              return TokenType.symbol_exclamation_point

      elseIf (ch == '$' )
        local id = read_identifier
        which (id)
          case "define":              add_new_token( TokenType.directive_define )
          case "include":             add_new_token( TokenType.directive_include )
          case "includeNativeCode":   add_new_token( TokenType.directive_includeNativeCode )
          case "includeNativeHeader": add_new_token( TokenType.directive_includeNativeHeader )
          case "if":                  add_new_token( TokenType.directive_if )
          case "elseIf":              add_new_token( TokenType.directive_elseIf )
          case "else":                add_new_token( TokenType.directive_else )
          case "endIf":               add_new_token( TokenType.directive_endIf )
          case "requisite":           add_new_token( TokenType.directive_requisite )
          others
            add_new_token( TokenType.placeholder_id, "$" + id )
        endWhich
        return null

      elseIf (ch == '%')
        if     (consume('=')) return TokenType.symbol_percent_equals
        else                  return TokenType.symbol_percent


      elseIf (ch == '&' )
        if     (consume('&')) throw error( "Use 'and' instead of '&&'." )
        elseIf (consume('=')) return TokenType.symbol_ampersand_equals
        else                  return TokenType.symbol_ampersand

      elseIf (ch == '(')
        return TokenType.symbol_open_paren

      elseIf (ch == ')')
      return TokenType.symbol_close_paren

      elseIf (ch == '*')
        if     (consume('=')) return TokenType.symbol_times_equals
        else                  return TokenType.symbol_times

      elseIf (ch == '+')
        if     (consume('=')) return TokenType.symbol_plus_equals
        elseIf (consume('+')) return TokenType.symbol_plus_plus
        else                  return TokenType.symbol_plus

      elseIf (ch == ',')
        return TokenType.symbol_comma

      elseIf (ch == '-')
        if     (consume('='))  return TokenType.symbol_minus_equals
        elseIf (consume('-'))  return TokenType.symbol_minus_minus
        elseIf (reader.peek(0) == '>' and reader.peek(1) == '>') return TokenType.symbol_minus
        elseIf (consume('>'))  return TokenType.symbol_arrow
        else                   return TokenType.symbol_minus

      elseIf (ch == '.' )
        if (consume('.'))
          if (consume('.'))
            # ellipsis
            while (consume(' ')) noAction
            if (not consume('\n')) throw error( "End of line expected after '...'." )
            return null 
          elseIf (consume('<'))
            return TokenType.symbol_upToLessThan
          elseIf (consume('>'))
            return TokenType.symbol_downToGreaterThan
          else
            return TokenType.symbol_upTo
          endIf
        elseIf (consume('='))
          return TokenType.symbol_dot_equals
        else
          return TokenType.symbol_dot
        endIf

      elseIf (ch == '/')
        if (consume('='))
          return TokenType.symbol_slash_equals
        else
          return TokenType.symbol_slash
        endIf

      elseIf (ch == ':')
        if (consume(':'))
          return TokenType.symbol_colon_colon
        endIf

        if (consume("<<:"))  return TokenType.symbol_shift_left
        elseIf (consume(">>:"))  return TokenType.symbol_shift_right
        elseIf (consume(">>>:")) return TokenType.symbol_shift_right_x
        return TokenType.symbol_colon

      elseIf (ch == ';')
        return TokenType.symbol_semicolon

      elseIf (ch == '<' )
        if     (consume('<')) return TokenType.symbol_open_specialize
        elseIf (consume('=')) return TokenType.symbol_le
        elseIf (consume('>')) return TokenType.symbol_compare
        else                  return TokenType.symbol_lt

      elseIf (ch == '=' )
        if (consume('='))     return TokenType.symbol_eq
        elseIf (consume('>')) return TokenType.symbol_fat_arrow
        else                  return TokenType.symbol_equals

      elseIf (ch == '>' )
        if      (consume('=')) return TokenType.symbol_ge
        elseIf  (consume('>')) return TokenType.symbol_close_specialize
        else                   return TokenType.symbol_gt

      elseIf (ch == '?')
        return TokenType.symbol_question_mark

      elseIf (ch == '@' )
        return TokenType.symbol_at

      elseIf (ch == '[')
        if (consume(']')) return TokenType.symbol_empty_brackets
        return TokenType.symbol_open_bracket
        #return check_for_directives( TokenType.symbol_open_bracket )

      elseIf (ch == '\\')
        return TokenType.symbol_backslash

      elseIf (ch == ']')
        return TokenType.symbol_close_bracket

      elseIf (ch == '^')
        if     (consume('=')) return TokenType.symbol_caret_equals
        else                  return TokenType.symbol_caret

      elseIf (ch == '{')
        if (consume('}')) return TokenType.symbol_empty_braces
        return TokenType.symbol_open_brace

      elseIf (ch == '|' )
        if     (consume('|')) throw error( "Use 'or' instead of '||'." )
        elseIf (consume('=')) return TokenType.symbol_vertical_bar_equals
        else                  return TokenType.symbol_vertical_bar

      elseIf (ch == '}' )
        if     (consume('#')) return TokenType.symbol_close_comment # used for error reporting
        else                  return TokenType.symbol_close_brace

      elseIf (ch == '~')
        if     (consume('=')) return TokenType.symbol_tilde_equals
        else                  return TokenType.symbol_tilde

      else
        throw error( "Unexpected input '"+ch+"'." )
      endIf


    method next_is_hex_digit->Logical
      local ch = reader.peek
      return (ch >= '0' and ch <= '9') or (ch >= 'a' and ch <= 'f') or (ch >= 'A' and ch <= 'F')

    method read_character->Character
      if (not reader.has_another) throw error( "Character expected." )

      local ch = reader.peek
      if (ch == '\n') throw error( "Character expected; found end of line." )

      if (ch == '\\')
        reader.read
        if (not reader.has_another) throw error( "Escaped character expected; found end of input." )

        if (consume('b')) return 8->Character
        if (consume('f')) return 12->Character
        if (consume('n')) return '\n'
        if (consume('r')) return '\r'
        if (consume('t')) return '\t'
        if (consume('v')) return 11->Character
        if (consume('0')) return '\0'
        if (consume('/')) return '/'
        if (consume('\''))return '\''
        if (consume('\\'))return '\\'
        if (consume('"')) return '"'
        if (consume('x')) return read_hex_value(2)
        if (consume('u')) return read_hex_value(4)
        throw error( "Invalid escape sequence.  Supported: \\n \\r \\t \\0 \\/ \\' \\\\ \\\" \\" + "uXXXX \\" + "xXX." )
      endIf

      local value = reader.read->Integer
      if ((value & 0x80) != 0)
        # Handle UTF8 encoding
        local ch2 = reader.read->Integer

        if ((value & 0x20) == 0)
          # %110xxxxx 10xxxxxx
          value = value & 0x1f
          ch2 = value & 0x3f
          return ((value:<<:6) | ch2)->Character
        else
          # %1110xxxx 10xxxxxx 10xxxxxx
          local ch3 = reader.read->Integer
          value = value & 15
          ch2 = ch2 & 0x3f
          ch3 = ch3 & 0x3f
          return ((value:<<:2) | (ch2:<<:6) | ch3)->Character
        endIf
      endIf
      return value->Character

    method read_hex_value( digits:Integer )->Character
      local value = 0
      local i = 1
      while (i <= digits)
        if (not reader.has_another) throw error( digits + "-digit hex value expected; found end of file." )
        if (not next_is_hex_digit)
          local ch = reader.peek
          local error_buffer = StringBuilder()
          error_buffer.print( "Invalid hex digit " )
          if (ch < ' ' or ch->Integer == 127) error_buffer.print( ch->Integer )
          else error_buffer.print( "'" + ch + "'" )
          error_buffer.print('.')
          throw error( error_buffer->String )
        endIf
        local intval = reader.read->Integer
        value = (value:<<:4) + intval
        ++i
      endWhile
      return value->Character

    method read_identifier->String
      buffer.clear
      local ch = reader.peek
      while ((ch>='a' and ch<='z') or (ch>='A' and ch<='Z') or (ch>='0' and ch<='9') or ch=='_')
        buffer.print( reader.read )
        ch = reader.peek
      endWhile

      if (buffer.count == 0) throw error( "Identifier expected." )

      return buffer->String

    method tokenize_alternate_string( terminator:Character )->Logical
      buffer.clear

      while (reader.has_another)
        if (reader.has_another)
          local ch = reader.peek
          if (ch == terminator)
            reader.read
            ch = reader.peek
            if (ch == terminator)
              reader.read
              return add_new_string_or_character_token_from_buffer(0)
            else
              buffer.print( terminator )
            endIf
          else
            buffer.print( read_character )
          endIf
        endIf
      endWhile

      throw error( "End of file reached while looking for end of string." )

    method tokenize_another->Logical
      reader.consume_spaces

      # Must be the has_another test for the terminating EOL
      next_filepath = filepath
      next_line = reader.line
      next_column = reader.column

      if (not reader.has_another) return false

      local ch = reader.peek
      if (ch == '\n') reader.read; return add_new_token( TokenType.eol )

      if (ch.is_letter or ch == '_')
        local id = read_identifier
        local keyword_type = TokenType.lookup[id]
        if (keyword_type)
          if (keyword_type is TokenType.keyword_nativeCode)
            return scan_native_code
          elseIf (keyword_type is TokenType.keyword_nativeHeader)
            return scan_native_header
          else
            return add_new_token( keyword_type )
          endIf
        else
          return add_new_token( TokenType.identifier, id )
        endIf
        return true

      elseIf (ch == '\'')
        if (reader.peek(1) == '\'')
          reader.read
          reader.read
          return tokenize_alternate_string( '\'' )
        else
          return tokenize_string( '\'' )
        endIf

      elseIf (ch == '"')
        return tokenize_string( '"' )

      elseIf (ch == '#')
        return tokenize_comment

      elseIf (ch >= '0' and ch <= '9')
        which (reader.peek(1))
          case 'b': return tokenize_integer_in_base(2)
          case 'c': return tokenize_integer_in_base(8)
          case 'x': return tokenize_integer_in_base(16)
          others:   return tokenize_number
        endWhich

      elseIf (ch == '@' and reader.peek(1) == '|')
        return tokenize_verbatim_string

      else
        # Either a symbol or a decimal like ".1".
        if (ch == '.')
          local next = reader.peek(1)
          if (next >= '0' and next <= '9') return tokenize_number
        endIf

        # Symbol
        local token_type = get_symbol_token_type
        if (token_type is null) return true
        return add_new_token( token_type )
      endIf

      local name = "'$'" (ch)
      if (ch == 10) name = "EOL"
      elseIf (ch < 32 or ch > 126) name = "(Unicode $)" (ch->Integer)

      throw error( "Syntax error - unexpected input $." (name) )

    method tokenize_comment->Logical
      buffer.clear
      reader.read  # '#'
      if (consume('{'))
        local nesting_count = 1
        while (reader.has_another)
          local ch = reader.read
          which (ch)
            case '#'
              buffer.print('#')
              if (consume('{'))
                buffer.print('{')
                ++nesting_count
              endIf

            case '}'
              if (consume('#'))
                --nesting_count
                if (nesting_count == 0) escapeWhile
                else buffer.print('}'); buffer.print('#')
              else
                buffer.print('}')
              endIf

            others
              buffer.print( ch )
          endWhich
        endWhile
      else
        while (reader.has_another and reader.peek != '\n') buffer.print( reader.read )
      endIf

      if (tokens.count and tokens.last.type is TokenType.eol)
        # Store comments in preceding EOL, if any.
        (tokens.last as EOLToken).comment += buffer->String
      endIf

      return true

    method tokenize_integer_in_base( base:Integer )->Logical
      reader.read  # '0'
      reader.read  # [b,c,x] = [2,8,16]

      #{
      local count = 0
      local digit = reader.peek.to_number
      while (reader.has_another(count+1) and digit != -1 and digit < base)
        ++count
        digit = reader.peek( count ).to_number
      endWhile

      if (base == 2 and count > 32
      }#

      local count = 0
      local n = 0
      local digit = reader.peek.to_number( base )
      while (reader.has_another and digit != -1)
        if (digit >= base) throw error( "Digit out of range for base " + base + "." )
        ++count
        n = n * base + digit
        reader.read
        digit = reader.peek.to_number( base )
      endWhile

      if (count == 0) throw error( "One or more digits expected." )

      #if (bits_per_value * count > 32) throw error( "Number too large for base " + base + "." )

      return add_new_token( TokenType.literal_integer, n )

    method tokenize_number->Logical
      local is_negative = consume('-')

      local i = 0
      while (reader.has_another(i+1) and reader.peek(i).is_number()) ++i
      local ch = reader.peek(i)
      local is_real = ((ch == '.' and reader.peek(i+1).is_number) or ch == 'e' or ch == 'E')

      if (is_real)
        # We have a real number
        local n = scan_real
        ch = reader.peek

        if (ch == '.')
          ch = reader.peek(1)
          if (ch >= '0' and ch <= '9')
            reader.read
            local start_pos = reader.position
            local fraction = scan_real
            n += fraction / 10.0^(reader.position - start_pos)
          elseIf (ch == '.')
            # Start of range
            if (is_negative) n = -n
            return add_new_token( TokenType.literal_integer, n->Integer )
          elseIf ((ch >= 'a' and ch <= 'z') or (ch >= 'A' and ch <= 'Z') or ch == '_')
            # E.g. 5.hash_code is (5).hash_code, not 5.0hashcode
            return add_new_token( TokenType.literal_integer, n->Integer )
          else
            if (is_negative) n = -n
            return add_new_token( TokenType.literal_real, n )
          endIf
        endIf

        if (consume('E') or consume('e'))
          local negative_exponent = consume('-')
          if (not negative_exponent) consume('+')
          local power = scan_real
          if (negative_exponent) n /= 10.0^power
          else                   n *= 10.0^power
        endIf

        if (is_negative) n = -n;
        return add_new_token( TokenType.literal_real, n )

      else
        # Integer or Long
        local n = scan_long
        if (is_negative) n = -n;

        if (n == n->Integer)
          return add_new_token( TokenType.literal_integer, n->Integer )
        else
          return add_new_token( TokenType.literal_long, n )
        endIf
      endIf

    method scan_real->Real
      local n = 0.0
      local ch = reader.peek
      while (ch >= '0' and ch <= '9')
        local intval = reader.read->Integer - '0'
        n = n * 10 + intval
        ch = reader.peek
      endWhile
      return n

    method scan_long->Long
      local n = 0 : Long
      local ch = reader.peek
      while (ch >= '0' and ch <= '9')
        local intval = reader.read->Integer - '0'
        n = n * 10 + intval
        ch = reader.peek
      endWhile
      return n

    method scan_native_code->Logical
      local buffer = StringBuilder()
      reader.consume_spaces
      if (reader.consume('\n'))
        # Multi-line native code ending with endNativeCode
        local found_end = false
        while (reader.has_another)
          if (reader.column == 1 and reader.consume_id("endNativeCode")) found_end = true; escapeWhile
          buffer.print( reader.read )
        endWhile
        if (not found_end) throw error( "'endNativeCode' expected before EOF." )
      else
        # Single line native code to EOL
        while (reader.has_another)
          if (reader.consume('\n')) escapeWhile
          buffer.print( reader.read )
        endWhile
      endIf

      forEach (line in LineReader(buffer->String))
        Program.native_code.add( line )
      endForEach

      return true

    method scan_native_header->Logical
      local buffer = StringBuilder()
      reader.consume_spaces
      if (reader.consume('\n'))
        # Multi-line native code ending with endNativeHeader
        local found_end = false
        while (reader.has_another)
          if (reader.column == 1 and reader.consume_id("endNativeHeader")) found_end = true; escapeWhile
          buffer.print( reader.read )
        endWhile
        if (not found_end) throw error( "'endNativeHeader' expected before EOF." )
      else
        # Single line native code to EOL
        while (reader.has_another)
          if (reader.consume('\n')) escapeWhile
          buffer.print( reader.read )
        endWhile
      endIf

      forEach (line in LineReader(buffer->String))
        Program.native_header.add( line )
      endForEach

      return true

    method tokenize_string( terminator:Character )->Logical
      buffer.clear
      reader.read
      while (reader.has_another)
        local ch = reader.peek
        if (ch == terminator)
          reader.read
          return add_new_string_or_character_token_from_buffer(terminator)
        else
          buffer.print( read_character )
        endIf
      endWhile

      throw error( "End of input reached while looking for end of string." )

    method tokenize_verbatim_string->Logical
      buffer.clear
      reader.read
      reader.read
      while (reader.has_another)
        local ch = reader.read
        if (ch == 10)
          consume_spaces
          if (consume('|'))
            buffer.print( ch )
          else
            add_new_token( TokenType.literal_string, buffer->String )
            return add_new_token( TokenType.eol )
          endIf
        else
          buffer.print( ch )
        endIf
      endWhile

      throw error( "End of File reached while looking for end of verbatim string." )
endClass

